#!/usr/bin/env python3
"""
IDR Extract Scripts Generator
=============================

This CLI script reads a JSON metadata file (generated by make_json_from_table_match.py)
and creates individual .py files containing IDROutputter subclasses for each table/view.

Each generated script contains:
- A complete IDROutputter subclass implementation
- SELECT query with all columns in Snowflake order
- Proper class and file naming conventions
- Ready-to-run or customizable implementation

Usage:
    python make_extract_scripts_from_json.py metadata.json output_directory/
    
Requirements:
    - JSON metadata file from make_json_from_table_match.py
    - Output directory (will be created if it doesn't exist)
    
Generated Files:
    - One .py file per table/view
    - File names: {table_name_lower}_export.py
    - Class names: {TableNameCamelCase}Exporter
"""

import json
import argparse
import os
from typing import Dict, List, Any
from datetime import datetime


def convert_to_camel_case(snake_str: str) -> str:
    """
    Convert snake_case or UPPER_CASE to CamelCase for class names.
    
    Args:
        snake_str: String in snake_case or UPPER_CASE format
        
    Returns:
        CamelCase string suitable for class naming
    """
    # Handle empty or None input
    if not snake_str:
        return ""
    
    # Split by underscores and capitalize each part
    components = snake_str.lower().split('_')
    return ''.join(word.capitalize() for word in components if word)


def convert_to_snake_case(input_str: str) -> str:
    """
    Convert string to snake_case for file naming.
    
    Args:
        input_str: Input string in any case format
        
    Returns:
        snake_case string suitable for file naming
    """
    return input_str.lower()


def generate_select_query(*, table_info: Dict[str, Any]) -> str:
    """
    Generate SELECT query string with all columns in Snowflake order.
    Includes SQL comments with type and description information.
    
    Args:
        table_info: Table information dictionary from JSON metadata
        
    Returns:
        Formatted SELECT query string with informative comments
    """
    database = table_info["database"]
    schema = table_info["schema"] 
    table_name = table_info["table_name"]
    columns = table_info["columns"]
    
    # Sort columns by ordinal_position to maintain Snowflake order
    sorted_columns = sorted(columns, key=lambda x: x["ordinal_position"])
    
    # Create column list with proper indentation and comments
    column_lines = []
    for i, col in enumerate(sorted_columns):
        column_name = col["column_name"]
        # Add comma except for the last column
        comma = "," if i < len(sorted_columns) - 1 else ""
        
        # Build the comment with type and description
        comment_parts = []
        
        # Add type description (e.g., VARCHAR(255), DECIMAL(15,2))
        if col.get("type_description"):
            comment_parts.append(col["type_description"])
        elif col.get("data_type"):
            comment_parts.append(col["data_type"])
        
        # Add column description/comment if available
        if col.get("comment"):
            comment_parts.append(col["comment"])
        
        # Combine comment parts
        if comment_parts:
            comment = " -- " + ": ".join(comment_parts)
        else:
            comment = ""
        
        column_lines.append(f"                {column_name}{comma}{comment}")
    
    columns_str = "\n".join(column_lines)
    
    return f"""SELECT
{columns_str}
            FROM {database}.{schema}.{table_name}"""


def generate_script_content(*, table_info: Dict[str, Any], metadata_info: Dict[str, Any]) -> str:
    """
    Generate complete .py script content for a table's IDROutputter subclass.
    
    Args:
        table_info: Table information dictionary from JSON metadata
        metadata_info: Metadata section from JSON file
        
    Returns:
        Complete Python script content as string
    """
    table_name = table_info["table_name"]
    database = table_info["database"]
    schema = table_info["schema"]
    table_type = table_info["table_type"]
    column_count = len(table_info["columns"])
    
    # Generate names
    class_name = convert_to_camel_case(table_name) + "Exporter"
    file_name_stub = convert_to_snake_case(table_name) + "_idr_export"
    
    # Generate SELECT query
    select_query = generate_select_query(table_info=table_info)
    
    # Get generation timestamp
    generated_date = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    original_pattern = metadata_info.get("search_pattern", "Unknown")
    
    return f'''"""
==========
{table_name} IDR Export (Auto-Generated)
==========

Auto-generated IDROutputter implementation for {database}.{schema}.{table_name}

Table Type: {table_type}
Columns: {column_count}
Generated: {generated_date}
Source Pattern: {original_pattern}

This script provides a complete IDROutputter subclass that exports all columns
from the table in their original Snowflake order. You can customize the
getSelectQuery() method to add WHERE clauses, JOINs, or modify column selection.

Usage:
    # In a Snowflake notebook environment:
    exporter = {class_name}()
    exporter.do_idr_output()
"""

# Import python packages
import pandas as pd
from datetime import datetime

# Import the IDROutputter base class
try:
    from IDROutputter import IDROutputter
except ImportError:
    print("Loading IDROutputter from previous cell or add proper import path")


class {class_name}(IDROutputter):
    """
    Exports {table_name} data using IDROutputter base class.
    
    Auto-generated from table metadata. Customize as needed for your specific requirements.
    
    Table: {database}.{schema}.{table_name} ({table_type})
    Columns: {column_count}
    """
    
    # Class properties
    version_number: str = "v01"
    file_name_stub: str = "{file_name_stub}"
    
    def getSelectQuery(self) -> str:
        """
        Returns the SELECT query for {table_name} export.
        
        Includes all {column_count} columns in their original Snowflake order.
        Customize this method to add WHERE clauses, JOINs, or modify column selection.
        """
        return """{select_query}"""


if __name__ == '__main__':
    # Execute the export using the IDROutputter framework
    exporter = {class_name}()
    exporter.do_idr_output()

# To download use: 
# snowsql -c cms_idr -q "GET @~/ file://. PATTERN='.*.csv';"
# Or look in ../misc_scripts/ for download_and_merge_all_snowflake_csv.sh
'''


def load_metadata_json(*, json_file_path: str) -> Dict[str, Any]:
    """
    Load and validate JSON metadata file.
    
    Args:
        json_file_path: Path to JSON metadata file
        
    Returns:
        Parsed JSON metadata dictionary
    """
    try:
        with open(json_file_path, 'r') as f:
            metadata = json.load(f)
        
        # Basic validation
        if "metadata" not in metadata:
            raise ValueError("JSON file missing 'metadata' section")
        if "tables" not in metadata:
            raise ValueError("JSON file missing 'tables' section")
        
        print(f"Loaded metadata for {len(metadata['tables'])} tables/views")
        return metadata
        
    except FileNotFoundError:
        print(f"make_extract_scripts_from_json Error: JSON file not found: {json_file_path}")
        raise
    except json.JSONDecodeError as e:
        print(f"make_extract_scripts_from_json Error: Invalid JSON format")
        print(f"JSON decode error: {str(e)}")
        raise
    except Exception as e:
        print(f"make_extract_scripts_from_json Error: Failed to load JSON file")
        print(f"Error details: {str(e)}")
        raise


def create_output_directory(*, output_dir: str) -> None:
    """
    Create output directory if it doesn't exist.
    
    Args:
        output_dir: Path to output directory
    """
    try:
        os.makedirs(output_dir, exist_ok=True)
        print(f"Output directory ready: {output_dir}")
    except Exception as e:
        print(f"make_extract_scripts_from_json Error: Failed to create output directory")
        print(f"Error details: {str(e)}")
        raise


def generate_all_scripts(*, metadata: Dict[str, Any], output_dir: str) -> None:
    """
    Generate all IDROutputter scripts from metadata.
    
    Args:
        metadata: Parsed JSON metadata dictionary
        output_dir: Directory to write generated scripts
    """
    tables = metadata["tables"]
    metadata_info = metadata["metadata"]
    
    if not tables:
        print("No tables found in metadata - no scripts to generate")
        return
    
    print(f"Generating {len(tables)} IDROutputter scripts...")
    
    generated_files = []
    
    for table_info in tables:
        table_name = table_info["table_name"]
        database = table_info["database"]
        schema = table_info["schema"]
        
        # Generate file name
        file_name = convert_to_snake_case(table_name) + "_export.py"
        file_path = os.path.join(output_dir, file_name)
        
        # Generate script content
        try:
            script_content = generate_script_content(
                table_info=table_info, 
                metadata_info=metadata_info
            )
            
            # Write script file
            with open(file_path, 'w') as f:
                f.write(script_content)
            
            generated_files.append({
                "file_name": file_name,
                "file_path": file_path,
                "table_name": table_name,
                "database": database,
                "schema": schema,
                "column_count": len(table_info["columns"])
            })
            
            print(f"Generated: {file_name} ({len(table_info['columns'])} columns)")
            
        except Exception as e:
            print(f"make_extract_scripts_from_json Error: Failed to generate script for {table_name}")
            print(f"Error details: {str(e)}")
            continue
    
    # Print summary
    print(f"\nGENERATION COMPLETE")
    print(f"==================")
    print(f"Scripts generated: {len(generated_files)}")
    print(f"Output directory: {output_dir}")
    print(f"Source metadata: {metadata_info.get('search_pattern', 'Unknown pattern')}")
    print(f"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    if generated_files:
        print(f"\nGenerated Files:")
        total_columns = 0
        for file_info in generated_files:
            print(f"  - {file_info['file_name']} ({file_info['column_count']} columns)")
            print(f"    Source: {file_info['database']}.{file_info['schema']}.{file_info['table_name']}")
            total_columns += file_info['column_count']
        
        print(f"\nTotal columns across all tables: {total_columns}")
        print(f"\nTo use these scripts:")
        print(f"1. Copy the generated .py files to your working directory")
        print(f"2. Run them in a Snowflake notebook environment")
        print(f"3. Customize the getSelectQuery() method as needed for filtering")


def main():
    """Main function for CLI usage"""
    parser = argparse.ArgumentParser(
        description="Generate IDROutputter scripts from JSON metadata",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
    python make_extract_scripts_from_json.py provider_metadata.json ./generated_scripts/
    python make_extract_scripts_from_json.py medicaid_tables.json ~/idr_exports/
    
The generated scripts will be named: {table_name}_export.py
Each script contains a complete IDROutputter subclass ready to use.
        """
    )
    
    parser.add_argument(
        'json_file',
        help="Path to JSON metadata file (from make_json_from_table_match.py)"
    )
    
    parser.add_argument(
        'output_directory',
        help="Directory to write generated .py scripts (will be created if needed)"
    )
    
    args = parser.parse_args()
    
    print("IDR Extract Scripts Generator")
    print("=============================")
    print(f"JSON File: {args.json_file}")
    print(f"Output Directory: {args.output_directory}")
    print("")
    
    try:
        # Load metadata from JSON file
        metadata = load_metadata_json(json_file_path=args.json_file)
        
        # Create output directory
        create_output_directory(output_dir=args.output_directory)
        
        # Generate all scripts
        generate_all_scripts(metadata=metadata, output_dir=args.output_directory)
        
        print(f"\nSuccess! All scripts generated successfully.")
        
    except Exception as e:
        print(f"\nFailed to generate scripts: {str(e)}")
        exit(1)


if __name__ == '__main__':
    main()
